{"cells":[{"cell_type":"markdown","metadata":{"id":"V8XfmRKMviRq"},"source":["## Assignment 6 : Reinforcement Learning<br>\n","Implement Reinforcement Learning using an example of a maze environment that the\n","agent needs to explore."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x_wD13LrKLE"},"outputs":[],"source":["import numpy as np\n","import random\n","\n","# Define the Maze\n","class Maze:\n","    def __init__(self):\n","        self.grid = np.array([[0, 0, 0, 0, 0],\n","                               [0, 1, 1, 1, 0],\n","                               [0, 1, 0, 0, 0],\n","                               [0, 1, 1, 1, 0],\n","                               [0, 0, 0, 0, 2]])\n","        self.start_state = (0, 0)  # Starting position\n","        self.goal_state = (4, 4)    # Goal position\n","        self.state = self.start_state\n","\n","    def reset(self):\n","        self.state = self.start_state\n","        return self.state\n","\n","    def step(self, action):\n","        x, y = self.state\n","\n","        if action == 0:  # Up\n","            x = max(0, x - 1)\n","        elif action == 1:  # Down\n","            x = min(4, x + 1)\n","        elif action == 2:  # Left\n","            y = max(0, y - 1)\n","        elif action == 3:  # Right\n","            y = min(4, y + 1)\n","\n","        if self.grid[x, y] == 1:  # If hitting a wall\n","            return self.state, -1, False  # Return current state, penalty, and not done\n","\n","        self.state = (x, y)\n","\n","        if self.state == self.goal_state:  # Reached the goal\n","            return self.state, 10, True  # Return goal state, reward, and done\n","        else:\n","            return self.state, -0.1, False  # Penalty for each step taken\n","\n","    def render(self):\n","        maze_copy = self.grid.copy()\n","        x, y = self.state\n","        maze_copy[x, y] = 3  # Mark the agent's position\n","        print(maze_copy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9WlkezKrfxW"},"outputs":[],"source":["class QLearningAgent:\n","    def __init__(self, maze):\n","        self.maze = maze\n","        self.q_table = np.zeros((5, 5, 4))  # 5x5 grid and 4 actions\n","        self.learning_rate = 0.1\n","        self.discount_factor = 0.9\n","        self.epsilon = 1.0  # Exploration rate\n","        self.epsilon_decay = 0.99\n","        self.min_epsilon = 0.1\n","        self.num_episodes = 1000\n","\n","    def choose_action(self, state):\n","        if random.uniform(0, 1) < self.epsilon:\n","            return random.randint(0, 3)  # Explore: choose random action\n","        else:\n","            return np.argmax(self.q_table[state[0], state[1]])  # Exploit: choose best action\n","\n","    def learn(self):\n","        for episode in range(self.num_episodes):\n","            state = self.maze.reset()\n","            done = False\n","\n","            while not done:\n","                action = self.choose_action(state)\n","                next_state, reward, done = self.maze.step(action)\n","\n","                # Update Q-table using the Q-learning formula\n","                best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n","                td_target = reward + self.discount_factor * self.q_table[next_state[0], next_state[1], best_next_action]\n","                td_delta = td_target - self.q_table[state[0], state[1], action]\n","                self.q_table[state[0], state[1], action] += self.learning_rate * td_delta\n","\n","                state = next_state\n","\n","            # Decay epsilon\n","            if self.epsilon > self.min_epsilon:\n","                self.epsilon *= self.epsilon_decay\n","\n","    def print_q_table(self):\n","        print(self.q_table)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":887,"status":"ok","timestamp":1728333102704,"user":{"displayName":"2209_SURAJ CHAUDHARI","userId":"02488172901437088190"},"user_tz":-330},"id":"RXrbpjm1rhq4","outputId":"0de0d00b-31cc-433e-b5f8-aa1c9cd00485"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[ 3.67891766  4.2612659   3.70524496  2.93099989]\n","  [ 0.25911092 -1.13448552  3.66160516 -0.24015041]\n","  [-0.23742012 -1.09635977 -0.28128027 -0.07940785]\n","  [-0.15046056 -1.06099571 -0.23713696  0.25047185]\n","  [-0.03695421  1.05294566 -0.16423133  0.04985236]]\n","\n"," [[ 3.38950147  4.845851    4.12813274  3.32961286]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.06086446  2.665014   -0.45584193  0.12069716]]\n","\n"," [[ 4.20683011  5.49539     4.61224376  3.73097334]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.19173019 -0.27273019 -0.19       -0.01802881]\n","  [-0.1        -0.19022438 -0.02883019  0.47994892]\n","  [ 0.30372228  5.22939365 -0.03048037  1.32718924]]\n","\n"," [[ 4.61724753  6.2171      5.41306523  4.53426366]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 1.05385325  8.33228183  3.25719889  3.29956633]]\n","\n"," [[ 5.43961937  6.13604322  6.13567625  7.019     ]\n","  [ 6.08682402  6.96578118  6.10345432  7.91      ]\n","  [ 6.95447301  7.87863888  6.94734021  8.9       ]\n","  [ 7.95823989  8.89090813  7.86284659 10.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}],"source":["if __name__ == \"__main__\":\n","    maze = Maze()\n","    agent = QLearningAgent(maze)\n","    agent.learn()\n","    agent.print_q_table()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPG650pTDSicGa8x7/7VO+g","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
